# ðŸ§ Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ollama Ð½Ð° Ubuntu Server

## ðŸ“‹ Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ

### ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ:
- **RAM:** 8GB (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ 16GB+)
- **CPU:** 4 ÑÐ´Ñ€Ð° (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ 8+ ÑÐ´ÐµÑ€)
- **Ð”Ð¸ÑÐº:** 20GB ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¼ÐµÑÑ‚Ð°
- **OS:** Ubuntu 20.04+ (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ Ubuntu 22.04 LTS)

### Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Llama 3.1:
- **RAM:** 16GB+ (Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ 8B), 40GB+ (Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ 70B)
- **CPU:** 8+ ÑÐ´ÐµÑ€
- **GPU:** NVIDIA GPU Ñ CUDA Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾, Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ)

## ðŸš€ Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ollama

### 1. ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹
```bash
sudo apt update && sudo apt upgrade -y
```

### 2. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹
```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ñ… Ð¿Ð°ÐºÐµÑ‚Ð¾Ð²
sudo apt install -y curl wget git build-essential

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Node.js (ÐµÑÐ»Ð¸ Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½)
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt install -y nodejs

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²ÐµÑ€ÑÐ¸Ð¸ Node.js
node --version
npm --version
```

### 3. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ollama
```bash
# Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¸ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸
ollama --version
```

### 4. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ollama ÐºÐ°Ðº ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ð¾Ð³Ð¾ ÑÐµÑ€Ð²Ð¸ÑÐ°

#### Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ Ollama
```bash
# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ ollama
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
sudo mkdir -p /usr/share/ollama/.ollama
sudo chown -R ollama:ollama /usr/share/ollama
```

#### Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ systemd ÑÐµÑ€Ð²Ð¸ÑÐ°
```bash
# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ„Ð°Ð¹Ð» ÑÐµÑ€Ð²Ð¸ÑÐ°
sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/bin:/usr/bin:/bin"
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_MODELS=/usr/share/ollama/.ollama/models"

[Install]
WantedBy=default.target
EOF

# ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ systemd
sudo systemctl daemon-reload

# Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑÐµÑ€Ð²Ð¸Ñ
sudo systemctl enable ollama
sudo systemctl start ollama

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ
sudo systemctl status ollama
```

## ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸

### 1. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Llama 3.1 8B (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ)
```bash
# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Llama 3.1 8B
ollama pull llama3.1:8b

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
ollama list
```

### 2. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Llama 3.1 70B (Ð´Ð»Ñ Ð¼Ð¾Ñ‰Ð½Ñ‹Ñ… ÑÐµÑ€Ð²ÐµÑ€Ð¾Ð²)
```bash
# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Llama 3.1 70B (Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ 40GB+ RAM)
ollama pull llama3.1:70b
```

### 3. ÐÐ»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
```bash
# Ð‘Ð¾Ð»ÐµÐµ Ð»ÐµÐ³ÐºÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÑÐ»Ð°Ð±Ñ‹Ñ… ÑÐµÑ€Ð²ÐµÑ€Ð¾Ð²
ollama pull llama3.1:3b

# Ð ÑƒÑÑÐºÐ¾ÑÐ·Ñ‹Ñ‡Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
ollama pull saiga:llama3.1_8b_instruct
```

## âš™ï¸ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ

### 1. ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ„Ð°Ð¹Ð»Ð° .env
```bash
# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ollama Ð² .env Ñ„Ð°Ð¹Ð»
echo "" >> .env
echo "# Ollama AI Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸" >> .env
echo "OLLAMA_HOST=http://localhost:11434" >> .env
echo "OLLAMA_MODEL=llama3.1:8b" >> .env
```

### 2. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð´Ð°ÐºÑˆÐµÐ½Ð°
```bash
# Ð”Ð»Ñ Ð²Ð½ÐµÑˆÐ½ÐµÐ³Ð¾ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ollama (ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾)
echo "OLLAMA_HOST=http://0.0.0.0:11434" >> .env
```

## ðŸ”§ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð´Ð°ÐºÑˆÐµÐ½Ð°

### 1. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ñ„Ð°Ð¹Ñ€Ð²Ð¾Ð»Ð°
```bash
# ÐžÑ‚ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ð¾Ñ€Ñ‚ 11434 Ð´Ð»Ñ Ollama
sudo ufw allow 11434

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ñ„Ð°Ð¹Ñ€Ð²Ð¾Ð»Ð°
sudo ufw status
```

### 2. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Nginx (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
```bash
# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ Ð¿Ñ€Ð¾ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ollama
sudo tee /etc/nginx/sites-available/ollama > /dev/null <<EOF
server {
    listen 80;
    server_name your-domain.com;

    location /ollama/ {
        proxy_pass http://localhost:11434/;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;
        
        # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ñ‹ Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
        proxy_read_timeout 300s;
        proxy_connect_timeout 75s;
    }
}
EOF

# Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
sudo ln -s /etc/nginx/sites-available/ollama /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx
```

### 3. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
```bash
# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð»Ñ Ð»Ð¾Ð³Ð¾Ð²
sudo mkdir -p /var/log/ollama
sudo chown ollama:ollama /var/log/ollama

# ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ systemd ÑÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/bin:/usr/bin:/bin"
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_MODELS=/usr/share/ollama/.ollama/models"
StandardOutput=journal
StandardError=journal
SyslogIdentifier=ollama

[Install]
WantedBy=default.target
EOF

# ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐµÑ€Ð²Ð¸Ñ
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

## ðŸ§ª Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸

### 1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ollama
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ ÑÐµÑ€Ð²Ð¸ÑÐ°
sudo systemctl status ollama

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ API
curl http://localhost:11434/api/tags

# Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
ollama run llama3.1:8b "ÐŸÑ€Ð¸Ð²ÐµÑ‚! ÐšÐ°Ðº Ð´ÐµÐ»Ð°?"
```

### 2. Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· API
```bash
# Ð¢ÐµÑÑ‚ Ñ‡ÐµÑ€ÐµÐ· curl
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.1:8b",
    "prompt": "ÐŸÑ€Ð¸Ð²ÐµÑ‚! ÐšÐ°Ðº Ð´ÐµÐ»Ð°?",
    "stream": false
  }'
```

### 3. Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
```bash
# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ ÑÐºÑ€Ð¸Ð¿Ñ‚
cat > test_ollama_performance.js <<EOF
const fetch = require('node-fetch');

async function testOllama() {
  const startTime = Date.now();
  
  try {
    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'llama3.1:8b',
        prompt: 'ÐžÐ±ÑŠÑÑÐ½Ð¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ñ‡Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚',
        stream: false
      })
    });
    
    const result = await response.json();
    const endTime = Date.now();
    
    console.log('Ð’Ñ€ÐµÐ¼Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°:', endTime - startTime, 'Ð¼Ñ');
    console.log('ÐžÑ‚Ð²ÐµÑ‚:', result.response);
  } catch (error) {
    console.error('ÐžÑˆÐ¸Ð±ÐºÐ°:', error);
  }
}

testOllama();
EOF

# Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ñ‚ÐµÑÑ‚
node test_ollama_performance.js
```

## ðŸ“Š ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ

### 1. ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²
```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° htop Ð´Ð»Ñ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð°
sudo apt install -y htop

# ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸
htop

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð¸ÑÐºÐ°
df -h

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ollama
ps aux | grep ollama
```

### 2. ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
```bash
# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° swap (ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾)
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² fstab Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

### 3. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° GPU (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)
```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° NVIDIA Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ð¾Ð² (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ GPU)
sudo apt install -y nvidia-driver-525
sudo reboot

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° CUDA Ð¿Ð¾ÑÐ»Ðµ Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
nvidia-smi

# Ollama Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ GPU ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½
ollama run llama3.1:8b "Ð¢ÐµÑÑ‚ GPU ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ"
```

## ðŸ”§ Ð£ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð½ÐµÐ¿Ð¾Ð»Ð°Ð´Ð¾Ðº

### 1. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ¾Ð¼
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð»Ð¾Ð³Ð¸ ÑÐµÑ€Ð²Ð¸ÑÐ°
sudo journalctl -u ollama -f

# ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑÐµÑ€Ð²Ð¸Ñ
sudo systemctl restart ollama

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿Ð¾Ñ€Ñ‚
sudo netstat -tlnp | grep 11434
```

### 2. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð°Ð¼ÑÑ‚Ð¸
free -h

# Ð•ÑÐ»Ð¸ Ð½Ðµ Ñ…Ð²Ð°Ñ‚Ð°ÐµÑ‚ Ð¿Ð°Ð¼ÑÑ‚Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¼ÐµÐ½ÑŒÑˆÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
ollama pull llama3.1:3b
```

### 3. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒÑŽ
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ CPU
top

# ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_MAX_LOADED_MODELS=1
```

### 4. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ ÑÐµÑ‚ÑŒÑŽ
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾Ñ€Ñ‚Ð°
telnet localhost 11434

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ„Ð°Ð¹Ñ€Ð²Ð¾Ð»
sudo ufw status
```

## ðŸ“ˆ ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ

### 1. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° (Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð½Ð°Ð³Ñ€ÑƒÐ·Ð¾Ðº)
```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Docker Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
sudo apt install -y docker.io
sudo systemctl enable docker
sudo systemctl start docker

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Docker Compose Ñ„Ð°Ð¹Ð»Ð°
cat > docker-compose.yml <<EOF
version: '3.8'
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    restart: unless-stopped

volumes:
  ollama_data:
EOF

# Ð—Ð°Ð¿ÑƒÑÐº Ñ‡ÐµÑ€ÐµÐ· Docker
docker-compose up -d
```

### 2. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° HAProxy Ð´Ð»Ñ Ð±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸
sudo apt install -y haproxy

# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ HAProxy
sudo tee /etc/haproxy/haproxy.cfg > /dev/null <<EOF
global
    daemon

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

frontend ollama_frontend
    bind *:80
    default_backend ollama_backend

backend ollama_backend
    balance roundrobin
    server ollama1 127.0.0.1:11434 check
    # Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ Ð±Ð¾Ð»ÑŒÑˆÐµ ÑÐµÑ€Ð²ÐµÑ€Ð¾Ð² Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
EOF

sudo systemctl enable haproxy
sudo systemctl start haproxy
```

## ðŸ”’ Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ

### 1. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
```bash
# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾ÐºÑÐ¸ Ñ Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸ÐµÐ¹
sudo apt install -y nginx

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ„Ð°Ð¹Ð»Ð° Ð¿Ð°Ñ€Ð¾Ð»ÐµÐ¹
sudo htpasswd -c /etc/nginx/.htpasswd ollama_user

# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Nginx Ñ Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸ÐµÐ¹
sudo tee /etc/nginx/sites-available/ollama-secure > /dev/null <<EOF
server {
    listen 80;
    server_name your-domain.com;

    location /ollama/ {
        auth_basic "Ollama Access";
        auth_basic_user_file /etc/nginx/.htpasswd;
        
        proxy_pass http://localhost:11434/;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;
    }
}
EOF
```

### 2. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° SSL (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ)
```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Certbot
sudo apt install -y certbot python3-certbot-nginx

# ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ SSL ÑÐµÑ€Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð°
sudo certbot --nginx -d your-domain.com

# ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐµÑ€Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ð²
sudo crontab -e
# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÑƒ:
# 0 12 * * * /usr/bin/certbot renew --quiet
```

## ðŸ“ ÐŸÐ¾Ð»ÐµÐ·Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹

### Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð¼
```bash
# Ð—Ð°Ð¿ÑƒÑÐº/Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°/Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐº
sudo systemctl start ollama
sudo systemctl stop ollama
sudo systemctl restart ollama

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ‚Ð°Ñ‚ÑƒÑÐ°
sudo systemctl status ollama

# ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð»Ð¾Ð³Ð¾Ð²
sudo journalctl -u ollama -f
```

### Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸
```bash
# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
ollama list

# Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
ollama rm llama3.1:8b

# ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
ollama pull llama3.1:8b

# Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸
ollama show llama3.1:8b
```

### ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³
```bash
# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²
htop
free -h
df -h

# Ð¡ÐµÑ‚ÐµÐ²Ñ‹Ðµ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ
sudo netstat -tlnp | grep 11434

# Ð›Ð¾Ð³Ð¸ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ
tail -f /var/log/ollama/ollama.log
```

## âœ… ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¾Ñ‡Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº

- [ ] Ollama ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
- [ ] ÐœÐ¾Ð´ÐµÐ»ÑŒ Llama 3.1:8b Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°
- [ ] Systemd ÑÐµÑ€Ð²Ð¸Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¸ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½
- [ ] ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹
- [ ] Ð¤Ð°Ð¹Ñ€Ð²Ð¾Ð» Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½
- [ ] Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾ÑˆÐ»Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾
- [ ] ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½
- [ ] Ð ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ðµ ÐºÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¾

## ðŸ†˜ ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°

ÐŸÑ€Ð¸ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ½Ð¾Ð²ÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼:

1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð»Ð¾Ð³Ð¸: `sudo journalctl -u ollama -f`
2. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ ÑÑ‚Ð°Ñ‚ÑƒÑ ÑÐµÑ€Ð²Ð¸ÑÐ°: `sudo systemctl status ollama`
3. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²: `htop`
4. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ ÑÐµÑ‚ÑŒ: `curl http://localhost:11434/api/tags`

---

**Ð“Ð¾Ñ‚Ð¾Ð²Ð¾!** ðŸŽ‰ Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ollama ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð½Ð° Ð²Ð°ÑˆÐµÐ¼ Ubuntu ÑÐµÑ€Ð²ÐµÑ€Ðµ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ Llama 3.1 Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….
